#### DISCLAIMER : THE VAST MAJORITY OF THE CODE IN THIS FILE WAS IMPLEMENTED 
#### FOR THE BAYES METHODS PROJECT. THE CODE WAS ADAPTED TO WORK WITH A PARTICLE
#### FILTER WHICH ESTIMATES THE LIKELIHOOD, BUT MOST OF THIS FILE SHOULD BE 
#### CONSIDERED AS WORK WHICH WAS UNDERTAKEN AS PART OF A DIFFERENT PROJECT.

library(mvtnorm)
library(parallel)
library(doParallel)
library(foreach)
library(package=Rfast,exclude = "rmvnorm")

################ PROPOSAL FUNCTIONS AND CORRELATION FINDING FUNCTIONS ##########

multvarNormProp <- function(xt, propPars, n=1){
  # purpose : A multivariate Gaussian random walk proposal for Met-Hastings
  #           MCMC
  # inputs  : xt       - The value of the chain at the previous time step 
  #           propPars - The correlation structure of the proposal
  return(rmvnorm(n, mean=xt, sigma=propPars))
}

FixedmultvarNormProp <- function(xt, propPars, indies, n=1){
  # purpose : A multivariate Gaussian random walk proposal for Met-Hastings
  #           MCMC
  # inputs  : xt       - The value of the chain at the previous time step 
  #           propPars - The correlation structure of the proposal
  vec<-array(NA,dim=c(n,length(xt)))
  vec[,indies]<-t(array(xt[indies],dim=c(length(indies),n)))
  vec[,-indies]<-rmvnorm(n, mean=xt[-indies], sigma=propPars[-indies,-indies])
  return(vec)
}

multvarPropUpdate <- function(chain){
  # purpose : Updates the covariance structure of a multivariate Gaussian 
  #           random walk proposal using the sample correlation of the samples.
  # inputs  : The chain, where columns are parameters, and rows are sets of
  #           simulatenous parameter updates
  return(cov(chain[,-1]))
}

######################## IMPLEMENTATION OF THE SAMPLER #########################
MH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
               uFunc=NULL, prntPars=FALSE){
  # purpose : Adaptive Metropolis hastings MCMC
  # inputs  : proposal  - A function which generates proposals for new points
  #           propPars  - Parameters for the proposal distribution
  #           lTarg     - A function which can evaluate the log target
  #           lTargPars - The parameters of the log-target
  #           x0        - A chosen point at which to commence to algorithm
  #           itermax   - The maximum number of points to propose
  #           uFunc     - If an update function is provided, adaptive MCMC
  #                       will be used instead.
  # output  : A list of points generated by the MCMC algorithm
  n <- length(x0) + 1
  xPrev <- x0
  
  # if there's no update function to do adaptive MCMC, we simply run the chain
  # normally and return the result:
  if (is.null(uFunc)){
    chain <- runChain(itermax, proposal, propPars, xPrev, lTarg, lTargPars)
    return(chain)
  }
  
  # otherwise, we perform adaptive MCMC:
  else{
    output <- matrix(c(lTarg(xPrev, lTargPars), xPrev), nrow=1, byrow=T)
    
    # We perform the MCMC in three phases. An inital warmup phase (lasting 25%
    # of itermax), follOwed by an adaptation phase, where we use the warmup
    # samples to modify our proposal, sample 25% of itermax more samples and
    # then modify the proposal again. The final stage uses the newest proposal
    # to produce samples for the last 50% of itermax iterations
    
    # Split itermax into 2 groups of 25% and one group of 50%:
    div <- c(round((itermax-1)/4))
    indices <- c(div, div, itermax-2*div)
    
    for (im in indices){
      # Optionally display the proposal parameters:
      if (prntPars) print(propPars)
      
      # run the chain:
      chain <- runChain(im, proposal, propPars, xPrev, lTarg, lTargPars)
      
      # add the samples to the output matrix
      output <- rbind(output, chain)
      
      # This line simply breaks out the loop if we don't have a following 
      # phase, to avoid the computational cost of updating our parameters:
      if(dim(output)[1]>=itermax) break
      
      # update the proposal parameters for the next phase:
      propPars <- uFunc(chain)
      
      # update xPrev for the next phase:
      # xPrev <- chain[im,-1]
      # Choose the next initial values to have the largest log-likelihood.
      xPrev <- output[which.max(output[,1]),-1]
    }
    return(output)
  }
}

pMH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
                uFunc=NULL, prntPars=FALSE, nChains=4, clNeeds, RcppFile=NULL,
                packages = "Rcpp", cores=detectCores()){
  # purpose : wrapper function which runs MCMC chains in parallel
  # inputs  : nChains   - The number of chains which should be run
  #           clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  # Create the clusters and initiate paralellisation:
  cl<-makeCluster(cores)
  registerDoParallel(cl)
  clusterExport(cl, c(c("MH","runChain"), clNeeds))
  
  
  # Do the parallel loop:
  ls <- foreach(i=1:nChains, .packages = packages) %dopar%{
    
    # source any required files:
    if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
    
    # then run the chain on the cluster:
    to.ls <- MH(proposal = proposal, propPars = propPars, lTarg = lTarg,
                lTargPars = lTargPars, x0 = x0, itermax = itermax,
                uFunc = uFunc, prntPars = prntPars)
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  try(stopCluster(cl), silent = T)
  class(ls) <- 'pMCMCoutput'
  return(ls)
}

runChain <- function(itermax, proposal, propPars, xPrev, lTarg, lTargPars){
  # purpose : subroutine of MH, which performs the running of the MCMC chain.
  #           Using this subroutine allows for a cleaner implementation of 
  #           adaptive metropolis hastings
  # proposal : multivariate normal distribution
  #            starts with initial guess xPrev and generates a new value near this point
  #            via a covariance linked through propCov or the identity matrix
  # propPars : covariance matrix for growth rate intercept, gradient, capture prob, etc
  # xPrev    : initial guess of values of growth rate intercept, ..., etc
  # lTarg    : IPM target distribution (log) function to calculate next step for population
  # lTargPars : params and functions such as prior distributions, normal function for growth rate sample
  #             this is used to propagate state t from state t+1
  n <- length(xPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  
  # Add the initial point, the first column of the output matrix is an 
  # evaluation of the log Target at the sampled point:
  output[1, ] <- c(lTarg(xPrev, lTargPars), xPrev)
  
  it <- 2
  while (it <= itermax){
    
    # generate a proposed point:
    xNew <- proposal(xPrev, propPars)
    
    # calculate the acceptance probability:
    lTargNew <- try(lTarg(xNew, lTargPars), silent=T)
    
    # If the parameter set is so bad that we can't work out its probability, 
    # skip it, and decrease the index to match:
    if (class(lTargNew)=='try-error') next
    
    lTargOld <- output[it-1, 1]
    alpha <- exp(lTargNew - lTargOld)
    
    # determine acceptance or rejection:
    u <- runif(1)
    if (u<=alpha) output[it,] <- c(lTargNew, xNew)
    else output[it,] <- c(lTargOld, xPrev)
    
    # update Xt-1
    xPrev <- output[it,2:n]
    
    # update iterator:
    it <- it + 1
  }
  
  return(output)
}

############### RELATED UTILS FOR DIAGNOSTICS AND PLOTTING #####################

plot.pMCMCoutput <- function(chains, names=NULL, cols=NULL, filePath=NULL, 
                             cex=1, ...){
  # purpose : Draws diagnostic plots of an MCMC chain given the sampled points
  # input   : The output of the MCMC chain, containing the sampled points, 
  #           and the evaluation of the log target at each sampled point, and
  #           a vector of names for the pairwise plots, as well as an optional
  #           selection of columns to plot.
  # output  : returns nothing, but produces plots
  
  # define some helper functions for plotting:
  plotCol <- function(x) plot(x, type='l', col='blue', cex.lab=cex,
                              cex.axis=cex, cex.main=cex)
  plotACF <- function(x) plot(acf(x, plot=F)$acf,
                              type='h',col='blue',ylab='',xlab='', cex.lab=cex, 
                              cex.lab=cex, cex.main=cex)
  
  # store the previous plotting settings:
  prevMfrow <- par('mfrow')
  
  # 
  if (!is.null(cols)){
    if (!(1 %in% cols)) cols <- c(1, cols)
    colPlot <- cols
    parNum <- length(cols) - 1
  }
  
  counter <- 1
  for(chain in chains){
    
    # set parNum, columns to plot and variable names:
    if(is.null(cols)){parNum <- dim(chain)[2]-1 ; colPlot <- 1:parNum}
    if (is.null(names)) names <- c("log Post", paste("var", 1:parNum))
    print(names)
    samps <-  dim(chain)[1]
    par(mfrow=c(parNum, 1))
    
    # plot trace:
    if (!is.null(filePath)) {
      pdf(paste(filePath, counter, ".PDF", sep=""), ...)
      par(mfrow=c(parNum, 1))
      counter <- counter + 1
    }
    
    try(apply(chain[,colPlot[-1]], 2, plotCol),silent = T)
    if (!is.null(filePath)) dev.off()
    
    # plot ACF:
    if (!is.null(filePath)){
      try(pdf(paste(filePath, counter, ".PDF", sep=""), ...), silent = T)
      par(mfrow=c(parNum, 1))
      counter <- counter + 1
    }
    
    try(apply(chain[,colPlot[-1]], 2, plotACF), silent = T)
    if (!is.null(filePath)) dev.off()
    
    # plot joint posteriors:
    class(chain) <- "matrix"
    df <- data.frame(chain[runif(1000,1,nrow(chain)), colPlot])
    names(df) <- names
    
    if (!is.null(filePath)) {
      try(pdf(paste(filePath, counter, ".PDF", sep=""), ...), silent = T)
      par(mfrow=c(parNum, 1))
      counter <- counter + 1
    }
    
    plot(df, pch=16,col=adjustcolor('black', alpha=0.2), cex.lab=cex)
    if (!is.null(filePath)) dev.off()
  }
  
  # reset the graphical parameters to what they were before the call:
  par(mfrow=prevMfrow)
}

thinMCMC <- function(pMCMCoutput, alpha=0.1, removeBI=FALSE){
  # purpose : takes the output of the pHM function, and thins the chain to the
  #           desired level
  # inputs  : pMCMCMoutput - the output list of chains from the pMH function
  #           alpha        - the percentage of points to keep
  #           removeBI     - a number between 0 and 1 which indicates how far 
  #                          through the chain the burn in ends
  
  nchains <- length(pMCMCoutput)
  
  for (i in 1:nchains){
    chain <- pMCMCoutput[[i]]
    if (!is.null(removeBI)){
      pMCMCoutput[[i]] <- chain[round(nrow(chain)*removeBI):nrow(chain),]
    }
    
    n <- nrow(pMCMCoutput[[i]])
    toKeep <- round(seq(1,n,length=round(alpha*n)))
    pMCMCoutput[[i]] <- pMCMCoutput[[i]][toKeep,]
  }
  
  return(pMCMCoutput)
}

getAcceptance <- function(pMCMCoutput, start = 1, end = NULL){
  # purpose : Calculates each chain's acceptance probability
  # inputs  : pMCMCoutput - the output list of chains from the pMH function
  #           start       - the sample at which we start considering the
  #                         acceptance probability
  # output  : A numeric vector with the acceptance proability of each chain
  
  n <- length(pMCMCoutput)
  output <- rep(NA, n)
  
  for (i in 1:n){
    chain <- pMCMCoutput[[i]]
    N <- nrow(chain)
    if (is.null(end)) end <- N
    output[i] <- chain[start:end,1] %>% diff %>% `!=`(0) %>% sum %>% `/`(N)
  }
  
  return(output)
}

grDiagnostic <- function(chains,plotty=T){
  # purpose : Uses the CODA library to conduct a Gelman-Rubin test for MCMC
  #           convergence
  # inputs  : A list conatining the reult of the MCMC chains, assuming that
  #           the first column gives the value of the log-posterior
  # output  : Plot of the G-R convergence diagnostic test statistic
  require(coda)
  thinnednl <- lapply(chains, function(x) x[,-1])
  combined <- thinnednl %>% lapply(FUN=mcmc) %>% do.call(what=mcmc.list)
  if(plotty) return(gelman.plot(combined))
  return(combined)
}

######################################################################################

Nested_MH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
                      clNeeds, RcppFile=NULL, prntPars=FALSE, packages = "Rcpp", cores=detectCores()){
  # purpose : wrapper function which runs MCMC chains in parallel
  # inputs  : nChains   - The number of chains which should be run
  #           clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  # source any required files:
  if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
  
  itermax<-ceiling(itermax/cores)*cores+1L
  xPrev<-x0
  n <- length(xPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  xNew <- matrix(NA, nrow=cores, ncol=n-1)
  lTargNew<-alpha<-c()
  
  # Add the initial point, the first column of the output matrix is an 
  # evaluation of the log Target at the sampled point:
  output[1, ] <- c(lTarg(xPrev, lTargPars), xPrev)
  print(output[1, ])
  #print("Initial Value in Nested_MH = ");print(output[1,])
  
  # Create the clusters and initiate paralellisation:
  cl<-makeCluster(cores,outfile="./Rcode/OUTPUT.txt")
  registerDoParallel(cl)
  clusterExport(cl, clNeeds)
  
  it <- 2
  while (it <= itermax){
    
    # if(any((it:(it+cores)*100)%%itermax==0)){
    print(paste0(round(it*100/itermax),"% done. LL = ",output[it-1,1]))
    #   saveRDS(output,file=paste0("./Rcode/saver_",round(it*100/itermax),"percent.Rdata"))
    # }
    
    # generate a proposed point:
    for (c in 1:cores){
      xNew[c,] <- proposal(xPrev, propPars) 
    }
    
    # calculate the acceptance probability:
    lTargNew <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
      
      to.lTargNew <- tryCatch({lTarg(xNew[c,], lTargPars)}, error=function(e) NA)
    }
    
    lTargOld <- output[it-1, 1]
    alpha <- exp(lTargNew - lTargOld)
    
    print("alpha")
    print(alpha)
    print("")
    
    # determine acceptance or rejection:
    u <- runif(cores)
    u2 <- runif(cores)
    lTtmp<-0
    
    for (c in 1:cores){
      # Check that the log-likelihood was actually calculated
      if(is.na(lTargNew[c])) {output[it+c-1,] <- output[it+c-2,] ; next}
      # MH acceptance-rejection:
      if (u[c]<=alpha[c] & exp(lTargNew[c]-lTtmp)>=u2[c]) {
        output[it+c-1,] <- c(lTargNew[c], xNew[c,])
        lTtmp<-lTargNew[c]
      }
      else output[it+c-1,] <- output[it+c-2,]
    }
    
    # update Xt-1
    xPrev <- output[it+cores-1,2:n]
    
    # update iterator:
    it <- it + cores
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  try(stopCluster(cl), silent = T)
  return(output)
  
}

Nested_pMH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
                       uFunc=NULL, prntPars=FALSE, clNeeds, RcppFile=NULL,
                       packages = "Rcpp", cores=detectCores()){
  
  if(itermax<200) uFunc<-NULL
  
  RNGkind("L'Ecuyer-CMRG")
  set.seed(201020)
  
  if (is.null(uFunc)){
    chain <- Nested_MH(proposal = proposal, propPars = propPars, lTarg = lTarg, 
                       lTargPars = lTargPars, x0 = x0, itermax = itermax,
                       clNeeds = clNeeds, RcppFile = RcppFile, packages = packages, cores = cores)
    return(chain)
  }
  
  # otherwise, we perform adaptive MCMC:
  else{
    output <- NULL
    
    # We perform the MCMC in three phases. An inital warmup phase (lasting 25%
    # of itermax), follOwed by an adaptation phase, where we use the warmup
    # samples to modify our proposal, sample 25% of itermax more samples and
    # then modify the proposal again. The final stage uses the newest proposal
    # to produce samples for the last 50% of itermax iterations
    
    # Split itermax into 2 groups of 25% and one group of 50%:
    div <- c(round((itermax-1)/4))
    indices <- c(div, div, itermax-2*div)
    covy<-1
    
    for (im in indices){
      # Optionally display the proposal parameters:
      # if (prntPars) print(propPars)
      
      # run the chain:
      chain <- Nested_MH(proposal = proposal, propPars = propPars, lTarg = lTarg, 
                         lTargPars = lTargPars, x0 = x0, itermax = im,
                         clNeeds = clNeeds, RcppFile = RcppFile, packages = packages, cores = cores)
      # add the samples to the output matrix
      output <- rbind(output, chain)
      
      # This line simply breaks out the loop if we don't have a following 
      # phase, to avoid the computational cost of updating our parameters:
      if(dim(output)[1]>=itermax) break
      
      # update the proposal parameters for the next phase:
      chain%<>%na.omit
      # To automate 
      # ichain <- sample(x = 1:nrow(chain), size = round(0.5*nrow(chain)), replace = F, prob = exp(chain[,1]-max(chain[,1])))
      # propPars <- uFunc(chain[ichain,])
      propPars <- uFunc(chain)
      
      print(paste0("COV Matrix - Max diagonal element :",round(max(diag(propPars)),digits = 3)))
      print(paste0("COV Matrix - Max off-diagonal element :",round(max(propPars - diag(diag(propPars))),digits = 3)))
      
      saveRDS(propPars,file=paste0("./saver_COV_",im,"_",covy,".Rdata"))
      covy<-covy+1
      
      # update xPrev for the next phase, using a random sample of one of the values used to calculate the covariance matrix 
      # xPrev <- chain[sample(ichain,1),-1]
      xPrev <- chain[im,-1]
      # Choose the next initial values to have the largest log-likelihood.
      # xPrev <- output[which.max(output[,1]),-1]
    }
    return(output)
  }
  
}

pM_GaA <- function(propCOV, lTarg, lTargPars, x0, itermax=1000,
                   prntPars=FALSE, clNeeds, RcppFile=NULL,
                   packages = "Rcpp", cores=detectCores(), 
                   Params=list(GreedyStart=200,Pstar=0.234, gamzy0=1, epsilon=2, minVar=1e-3)){
  # purpose : Generalised Adaptative Metropolis-Hastings Algorithm with Global Adaptative Scaling
  # Details : Christian L. Muller, (ETH) 'Exploring the common concepts of a-MCMC and cov matrix
  #           adaptation schemes', Dagstuhl Seminar Proceedings, 2010.
  #           Uses the multivariate Gaussian distribution as proposal, 
  #           whereby the proposal covariance is updated at every iteration.
  # inputs  : clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  # RNGkind("L'Ecuyer-CMRG")
  seed<-as.numeric(stringr::str_flatten(unlist(strsplit(strsplit(as.character(Sys.time()),split = " ")[[1]][2],":"))))
  set.seed(round(runif(1,0,10000)*seed))
  
  propMu<- x0
  normCOV<-sum(propCOV*t(propCOV))
  gamzy <- Params$gamzy0/(1:(itermax+cores))^(seq(from=1/(1+Params$epsilon),to=1,length.out=(itermax+cores)))
  # gamzy<-exp(-(1:1000 - Params$burnin)/100)
  # gamzy<-exp(-(1:1000)^2/10000)
  # gamzy<-1/1:(itermax+cores)
  Lgsf<-0
  Pstar<-Params$Pstar
  # print("sum(<COV,COV>):")  
  # print(sum(propCOV*t(propCOV)))
  # print("")
  
  # source any required files:
  if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
  
  itermax<-ceiling(itermax/cores)*cores+1L
  xPrev<-x0
  n <- length(xPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  xNew <- matrix(NA, nrow=cores, ncol=n-1)
  lTargNew<-alpha<-c()
  
  # Add the initial point, the first column of the output matrix is an 
  # evaluation of the log Target at the sampled point:
  output[1, ] <- c(lTarg(xPrev, lTargPars), xPrev)
  print(output[1,1])
  #print("Initial Value in Nested_MH = ");print(output[1,])
  # Create the clusters and initiate paralellisation:
  # cl<-makeCluster(cores,outfile="./Rcode/OUTPUT.txt")
  # registerDoParallel(cl)
  # clusterExport(cl, clNeeds)
  
  it <- 2
  while (it <= itermax){
    
    # if(any((it:(it+cores)*100)%%itermax==0)){
    # print(paste0(round(it*100/itermax),"% done. LL = ",output[it-1,1]))
    # print(paste0(it*100/itermax,"% done. LL = ",output[it-1,1]))
    # saveRDS(output,file=paste0("./Rcode/saver_",round(it*100/itermax),"percent.Rdata"))
    # }
    
    # generate a proposed point using the MVN proposal and global scaling factor
    xNew <- multvarNormProp(xt=xPrev, propPars=exp(2*Lgsf)*propCOV*normCOV/sum(propCOV*t(propCOV)), n=cores) 
    
    # calculate the acceptance probability:
    # lTargNew <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
    #   # to.lTargNew <- tryCatch({lTarg(xNew[c,], lTargPars)}, error=function(e) NA)
    #   to.lTargNew <- lTarg(xNew[c,], lTargPars)
    # }
    
    lTargNew <- unlist(mclapply(X = 1:cores,
                                FUN = function(i) lTarg(xNew[i,], lTargPars),
                                mc.cores = cores))
        
    lTargOld <- output[it-1, 1]
    alpha <- apply(cbind(exp(lTargNew - lTargOld),rep(1,cores)),1,min)
    
    print(paste0("<alpha> = ",mean(alpha)))
    # print("New LL:")
    # print(lTargNew)
    
    # determine acceptance or rejection:
    u <- runif(cores)
    u2 <- runif(cores)
    
    tLgsf<-tpropMu<-tpropCOV<-nnz<-0
    lTtmp<-lTargOld
    
    for (c in 1:cores){
      # Check that the log-likelihood was actually calculated
      if(is.na(lTargNew[c])) {
        output[it+c-1,] <- output[it+c-2,]
        nnz<-nnz+1
        next
      }
      
      if (exp(lTargNew[c]-lTtmp)>=u[c]) { # Accepted!
        output[it+c-1,] <- c(lTargNew[c], xNew[c,])
        lTtmp<-lTargNew[c]
        xPrev<-xNew
      } else { # Rejected!
        output[it+c-1,] <- output[it+c-2,]
        # Greedy Start Adaptive Metropolis (AM) algorithm
        # (Only uses accepted parameters for GSF, mean & covariance updates at the start)
        if(it+c<=Params$GreedyStart) {nnz<-nnz+1; next}
      }
      
      # Global Scaling Factor (GSF), mean & covariance update
      tLgsf <- tLgsf + gamzy[it]*(alpha[c]-Pstar)
      tpropMu <- tpropMu + gamzy[it]*(xNew[c,] - propMu)
      tpropCOV <- tpropCOV + gamzy[it]*((xNew[c,] - propMu)%*%t(xNew[c,] - propMu) - propCOV)
      
    }
    
    # if(nnz<cores & it>Params$burnin){
    if(nnz<cores){
      # Add a minimum variance value to ensure global scale factor doesn't 
      # diag(tpropCOV)<-diag(tpropCOV)+Params$minVar
      
      Lgsf<-Lgsf+tLgsf/max(c(cores-nnz,1))
      propMu<-propMu+tpropMu/max(c(cores-nnz,1))
      propCOV<-propCOV+tpropCOV/max(c(cores-nnz,1))
    }

    # Lgsf<-max(Lgsf,Params$minVar)
    
    # print("sum(<COV,COV>):")  
    print(paste0("Total covariance = ",sum(exp(2*Lgsf)*propCOV*normCOV/sum(propCOV*t(propCOV)))))
    print(paste0("Global Scaling Factor, r = ",exp(2*Lgsf)))
    # print("")
    # if(any((it:(it+cores)*100)%%itermax==0)){
    print(paste0(round(it*100/itermax),"% done. LL = ",output[it,1]))
    # saveRDS(output,file=paste0("./Rcode/saver_",round(it*100/itermax),"percent.Rdata"))
    # }
    
    # update Xt-1
    xPrev <- output[it+cores-1,-1]
    # update iterator:
    it <- it + cores
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  # try(stopCluster(cl), silent = T)
  return(output)
  
}

p_HMC <- function(propCOV, lTarg, lTargPars, x0, itermax=1000,
                  prntPars=FALSE, clNeeds, RcppFile=NULL,
                  packages = "Rcpp", cores=detectCores(), 
                  Params=list(Pstar=0.65, Leaps=20, stepsize=0.0005, gamzy0=1, egam=2, minVar=1e-3)){
  # purpose : Generalised Adaptative Metropolis-Hastings Algorithm with Global Adaptative Scaling
  # Details : Christian L. Muller, (ETH) 'Exploring the common concepts of a-MCMC and cov matrix
  #           adaptation schemes', Dagstuhl Seminar Proceedings, 2010.
  #           Uses the multivariate Gaussian distribution as proposal, 
  #           whereby the proposal covariance is updated at every iteration.
  # inputs  : clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  ############## HAMILTONIAN MONTE-CARLO ALGORITHM #############
  # For more information, see either: 
  #           https://arxiv.org/pdf/1701.02434.pdf
  #           https://www.mcmchandbook.net/HandbookChapter5.pdf
  ### FOR EACH ITERATION 
  # Generate new p values
  # Sample new q values
  ### HMC-loop (leapfrog steps)
  # Propagate q & p (calculating dV/dq each time)
  ###
  # Reverse p
  # Calculate new Hamiltonian
  # Calculate old Hamiltonian
  # MH accept-reject
  # Adjust propCOV to ensure Pstar = 0.65
  ###
  ##############################################################
  # Define kinetic energy function - Kin - we use quadratic with parameter scaling metric:
  KinF  <- function(p, propCOV){
    # NOTE: negative log term corresponds to taking 1/det(propCOV)
    # propCOV = M^(-1) for M, the Euclidean metric (also called mass matrix)
    return(-0.5*sum(p%*%propCOV*p) - log(det(propCOV)))
    # return(-0.5*sum(p*propCOV*propCOV*p) - log(prod(propCOV*propCOV)))
  }
  # Define potential energy function - Vpot - we use logTargetIPM:
  VpotF <- function(q){
    return(-lTarg(q,lTargPars))
  }
  # Define the Hamiltonian - Ham
  HamF <- function(q,p,q_old=NULL,p_old=NULL,propCOV){
    # LL is actually -Vpotential, see output
    if(is.null(q_old)|is.null(p_old)) return(list(new=(VpotF(q)+KinF(p,propCOV))))
    return(list(new=(VpotF(q)+KinF(p,propCOV)),old=(VpotF(q_old)+KinF(p_old,propCOV))))
  }
  # Define derivative of potential dV/dq:
  dVpotF <- function(q,delta=0.0001){
    n<-length(q)
    dVpot<-rep(0,n)
    
    for (i in 1:n){
      delvec<-rep(0,n)
      delvec[i]=delta
      
      lower<-VpotF(q-delvec)
      upper<-VpotF(q+delvec)
      # print(mean(c(lower,upper)))
      
      dVpot[i]<-upper-lower
    }
    
    return(dVpot=dVpot/(2*delta))
  }
  # Define Leapfrog algorithm
  LeapfrogMe <- function(q,p,Leaps=20,eps=0.0005){
    p%<>%as.numeric()
    q%<>%as.numeric()
    for (L in 1:Leaps){
      # Half step in momentum
      dV1<-dVpotF(q)
      print("dVpot 1:")
      print(dV1)
      p<-p-0.5*eps*dV1
      print("0.5*eps*dV1:")
      print(0.5*eps*dV1)
      print("eps*p")
      print(eps*p)
      # Update position
      q<-q+eps*p
      print("q*")
      print(q)
      # Second half-step in momentum
      dV2<-dVpotF(q)
      print("dVpot 2:")
      print(dV2)
      p<-p-0.5*eps*dV2
    }
    return(list(q=q,p=-p))
  }
  
  # Add these functions to the required parallel functions (see ClusterExport)
  clNeeds%<>%c("lTarg","dVpotF","HamF","VpotF","KinF","LeapfrogMe")
  
  #%%%%%%%%%%%%%%%% WHEN TO PARALLELISE????? %%%%%%%%%%%%%%%%%#
  # Do they share propCOV? Do we make propCOV an average/median of the others?
  # Deal with step size and number of steps
  
  RNGkind("L'Ecuyer-CMRG")
  set.seed(201020)
  # source any required files:
  if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
  itermax<-ceiling(itermax/cores)*cores+1L
  qPrev<-x0
  n <- length(qPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  qNew <- matrix(NA, nrow=cores, ncol=n-1)
  # Create the clusters and initiate paralellisation:
  # cl<-makeCluster(cores,outfile="./Rcode/OUTPUT.txt")
  # registerDoParallel(cl)
  # clusterExport(cl, clNeeds)
  
  ### Adaptative accept-reject algorithm parameters, evolving covariance matrix ###
  propMu<- x0
  gamzy <- Params$gamzy0/(1:(itermax+cores))^(seq(from=1/(1+Params$egam),to=1,length.out=(itermax+cores)))
  Lgsf<-0
  Pstar<-Params$Pstar
  
  pInit<-multvarNormProp(rep(0,(n-1)), exp(2*Lgsf)*propCOV, n=cores)
  # Print out LL & summary statistics of the initial parameters
  print(-VpotF(qPrev))
  print(HamF(q = qPrev,p = pInit,propCOV = propCOV))
  Leapy<-LeapfrogMe(q = qPrev,p = pInit,Leaps = 3,eps = sqrt(diag(propCOV)))
  
  print("sum(<COV,COV>):")  
  print(sum(propCOV*t(propCOV)))
  print("")
  
  it <- 1
  while (it <= itermax){
    
    # Generate momentum variables:
    pCur <- multvarNormProp(rep(0,(n-1)), exp(2*Lgsf)*propCOV, n=cores)
    # pNew <- array(rnorm((n-1)*cores,mean=0,sd=1),dim=c((n-1),cores))*s
    
    # Generate a proposed point using the MVN proposal and global scaling factor
    qCur <- qPrev
    # qCur <- multvarNormProp(qPrev, exp(2*Lgsf)*propCOV, n=cores) 
    
    # calculate the HMC new position-momentum values:
    # listQPspace <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
    #   to.listQPspace <- tryCatch({LeapfrogMe(qCur[c,],pCur[c,],Leaps = Params$Leaps,
    #                                          eps = Params$stepsize)}, error=function(e) NA)
    # }
    listQPspace<-tryCatch({LeapfrogMe(qCur,pCur,Leaps = Params$Leaps,
                                      eps = sqrt(diag(propCOV)))}, error=function(e) NA)
    print("3")
    print(listQPspace)
    qNew<-listQPspace$q
    # NOTE: momentum is reversed for volume preservation!
    pNew<-listQPspace$p
    print("4")
    # Calculate the proposed and current Hamiltonian values
    # list_Ham <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
    #   to.list_Ham <- tryCatch({HamF(qNew[c,],pNew[c,],qCur[c,],pCur[c,],propCOV)}, error=function(e) NA)
    # }
    list_Ham<-tryCatch({HamF(qNew,pNew,qCur,pCur,propCOV)}, error=function(e) NA)
    Hamiltonian <- list_Ham$new
    o_Hamiltonian <- list_Ham$old
    print("5")
    # MH accept-reject parameter
    alpha <- apply(cbind(exp(-Hamiltonian + o_Hamiltonian),rep(1,cores)),1,min)
    print("<alpha> = ")
    print(alpha)
    print("")
    
    # determine acceptance or rejection:
    u <- runif(cores)
    
    tLgsf<-tpropMu<-tpropCOV<-nnz<-0
    HamTmp<-min(o_Hamiltonian)
    
    for (c in 1:cores){
      # Check that the log-likelihood was actually calculated
      if(is.na(Hamiltonian[c])) {
        output[it+c-1,] <- output[it+c-2,]
        nnz<-nnz+1
        next
      }
      
      # Note: modifying global scale factor by alpha, not exp(-Ham[c]+HamTmp)
      tLgsf <- tLgsf + gamzy[it]*(alpha[c]-Pstar)
      tpropMu <- tpropMu + gamzy[it]*(qCur[c,] - propMu)
      tpropCOV <- tpropCOV + gamzy[it]*((qCur[c,] - propMu)%*%t(qCur[c,] - propMu) - propCOV)
      
      # MH acceptance-rejection:
      if (exp(-Hamiltonian[c]+HamTmp)>=u[c]) {
        output[it+c-1,] <- c(Hamiltonian[c], qNew[c,])
        HamTmp<-Hamiltonian[c]
      } else output[it+c-1,] <- output[it+c-2,]
      
    }
    
    # # if(nnz<cores & it>Params$burnin){
    # if(nnz<cores){
    #   # Ensure a minimum variance value, even despite potentially low acceptance rate.
    #   diag(tpropCOV)<-diag(tpropCOV)+Params$minVar
    #   
    #   Lgsf<-Lgsf+tLgsf/max(c(cores-nnz,1))
    #   propMu<-propMu+tpropMu/max(c(cores-nnz,1))
    #   propCOV<-propCOV+tpropCOV/max(c(cores-nnz,1))
    # }
    
    # print("sum(<COV,COV>):")  
    # print(sum(propCOV*t(propCOV)))
    # print("Global Scaling Factor, r:")
    # print(exp(Lgsf))
    # print("")
    # if(any((it:(it+cores)*100)%%itermax==0)){
    print(paste0(round(it*100/itermax),"% done. LL = ",output[it,1]))
    print(output[it,-1])
    print("")
    # saveRDS(output,file=paste0("./Rcode/saver_",round(it*100/itermax),"percent.Rdata"))
    # }
    
    # update Xt-1
    qPrev <- output[it+cores-1,-1]
    
    # update iterator:
    it <- it + cores
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  # try(stopCluster(cl), silent = T)
  return(output)
  
}
