#### DISCLAIMER : THE VAST MAJORITY OF THE CODE IN THIS FILE WAS IMPLEMENTED 
#### FOR THE BAYES METHODS PROJECT. THE CODE WAS ADAPTED TO WORK WITH A PARTICLE
#### FILTER WHICH ESTIMATES THE LIKELIHOOD, BUT MOST OF THIS FILE SHOULD BE 
#### CONSIDERED AS WORK WHICH WAS UNDERTAKEN AS PART OF A DIFFERENT PROJECT.

library(parallel)
library(doParallel)
library(foreach)
library(package=Rfast)
if(algorithm=="ABCSIR") source(paste0(directory,"./Rcode/PerturbABC.R"))

################ PROPOSAL FUNCTIONS AND CORRELATION FINDING FUNCTIONS ##########

multvarNormProp <- function(xt, propPars, n=1){
  # purpose : A multivariate Gaussian random walk proposal for Met-Hastings
  #           MCMC
  # inputs  : xt       - The value of the chain at the previous time step 
  #           propPars - The correlation structure of the proposal
  return(rmvnorm(n, mean=xt, sigma=propPars))
}

FixedmultvarNormProp <- function(xt, propPars, indies, n=1){
  # purpose : A multivariate Gaussian random walk proposal for Met-Hastings
  #           MCMC
  # inputs  : xt       - The value of the chain at the previous time step 
  #           propPars - The correlation structure of the proposal
  vec<-array(NA,dim=c(n,length(xt)))
  vec[,indies]<-t(array(xt[indies],dim=c(length(indies),n)))
  vec[,-indies]<-rmvnorm(n, mean=xt[-indies], sigma=propPars[-indies,-indies])
  return(vec)
}

multvarPropUpdate <- function(chain){
  # purpose : Updates the covariance structure of a multivariate Gaussian 
  #           random walk proposal using the sample correlation of the samples.
  # inputs  : The chain, where columns are parameters, and rows are sets of
  #           simulatenous parameter updates
  return(cov(chain[,-1]))
}

######################## IMPLEMENTATION OF THE SAMPLER #########################
MH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
               uFunc=NULL, prntPars=FALSE){
  # purpose : Adaptive Metropolis hastings MCMC
  # inputs  : proposal  - A function which generates proposals for new points
  #           propPars  - Parameters for the proposal distribution
  #           lTarg     - A function which can evaluate the log target
  #           lTargPars - The parameters of the log-target
  #           x0        - A chosen point at which to commence to algorithm
  #           itermax   - The maximum number of points to propose
  #           uFunc     - If an update function is provided, adaptive MCMC
  #                       will be used instead.
  # output  : A list of points generated by the MCMC algorithm
  n <- length(x0) + 1
  xPrev <- x0
  
  # if there's no update function to do adaptive MCMC, we simply run the chain
  # normally and return the result:
  if (is.null(uFunc)){
    chain <- runChain(itermax, proposal, propPars, xPrev, lTarg, lTargPars)
    return(chain)
  }
  
  # otherwise, we perform adaptive MCMC:
  else{
    output <- matrix(c(lTarg(xPrev, lTargPars), xPrev), nrow=1, byrow=T)
    
    # We perform the MCMC in three phases. An inital warmup phase (lasting 25%
    # of itermax), follOwed by an adaptation phase, where we use the warmup
    # samples to modify our proposal, sample 25% of itermax more samples and
    # then modify the proposal again. The final stage uses the newest proposal
    # to produce samples for the last 50% of itermax iterations
    
    # Split itermax into 2 groups of 25% and one group of 50%:
    div <- c(round((itermax-1)/4))
    indices <- c(div, div, itermax-2*div)
    
    for (im in indices){
      # Optionally display the proposal parameters:
      if (prntPars) print(propPars)
      
      # run the chain:
      chain <- runChain(im, proposal, propPars, xPrev, lTarg, lTargPars)
      
      # add the samples to the output matrix
      output <- rbind(output, chain)
      
      # This line simply breaks out the loop if we don't have a following 
      # phase, to avoid the computational cost of updating our parameters:
      if(dim(output)[1]>=itermax) break
      
      # update the proposal parameters for the next phase:
      propPars <- uFunc(chain)
      
      # update xPrev for the next phase:
      # xPrev <- chain[im,-1]
      # Choose the next initial values to have the largest log-likelihood.
      xPrev <- output[which.max(output[,1]),-1]
    }
    return(output)
  }
}

pMH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
                uFunc=NULL, prntPars=FALSE, nChains=4, clNeeds, RcppFile=NULL,
                packages = "Rcpp", cores=detectCores()){
  # purpose : wrapper function which runs MCMC chains in parallel
  # inputs  : nChains   - The number of chains which should be run
  #           clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  # Create the clusters and initiate paralellisation:
  cl<-makeCluster(cores)
  registerDoParallel(cl)
  clusterExport(cl, c(c("MH","runChain"), clNeeds))
  
  
  # Do the parallel loop:
  ls <- foreach(i=1:nChains, .packages = packages) %dopar%{
    
    # source any required files:
    if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
    
    # then run the chain on the cluster:
    to.ls <- MH(proposal = proposal, propPars = propPars, lTarg = lTarg,
                lTargPars = lTargPars, x0 = x0, itermax = itermax,
                uFunc = uFunc, prntPars = prntPars)
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  try(stopCluster(cl), silent = T)
  class(ls) <- 'pMCMCoutput'
  return(ls)
}

runChain <- function(itermax, proposal, propPars, xPrev, lTarg, lTargPars){
  # purpose : subroutine of MH, which performs the running of the MCMC chain.
  #           Using this subroutine allows for a cleaner implementation of 
  #           adaptive metropolis hastings
  # proposal : multivariate normal distribution
  #            starts with initial guess xPrev and generates a new value near this point
  #            via a covariance linked through propCov or the identity matrix
  # propPars : covariance matrix for growth rate intercept, gradient, capture prob, etc
  # xPrev    : initial guess of values of growth rate intercept, ..., etc
  # lTarg    : IPM target distribution (log) function to calculate next step for population
  # lTargPars : params and functions such as prior distributions, normal function for growth rate sample
  #             this is used to propagate state t from state t+1
  n <- length(xPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  
  # Add the initial point, the first column of the output matrix is an 
  # evaluation of the log Target at the sampled point:
  output[1, ] <- c(lTarg(xPrev, lTargPars), xPrev)
  
  it <- 2
  while (it <= itermax){
    
    # generate a proposed point:
    xNew <- proposal(xPrev, propPars)
    
    # calculate the acceptance probability:
    lTargNew <- try(lTarg(xNew, lTargPars), silent=T)
    
    # If the parameter set is so bad that we can't work out its probability, 
    # skip it, and decrease the index to match:
    if (class(lTargNew)=='try-error') next
    
    lTargOld <- output[it-1, 1]
    alpha <- exp(lTargNew - lTargOld)
    
    # determine acceptance or rejection:
    u <- runif(1)
    if (u<=alpha) output[it,] <- c(lTargNew, xNew)
    else output[it,] <- c(lTargOld, xPrev)
    
    # update Xt-1
    xPrev <- output[it,2:n]
    
    # update iterator:
    it <- it + 1
  }
  
  return(output)
}

############### RELATED UTILS FOR DIAGNOSTICS AND PLOTTING #####################

plot.pMCMCoutput <- function(chains, names=NULL, cols=NULL, filePath=NULL, 
                             cex=1, ...){
  # purpose : Draws diagnostic plots of an MCMC chain given the sampled points
  # input   : The output of the MCMC chain, containing the sampled points, 
  #           and the evaluation of the log target at each sampled point, and
  #           a vector of names for the pairwise plots, as well as an optional
  #           selection of columns to plot.
  # output  : returns nothing, but produces plots
  
  # define some helper functions for plotting:
  plotCol <- function(x) plot(x, type='l', col='blue', cex.lab=cex,
                              cex.axis=cex, cex.main=cex)
  plotACF <- function(x) plot(acf(x, plot=F)$acf,
                              type='h',col='blue',ylab='',xlab='', cex.lab=cex, 
                              cex.lab=cex, cex.main=cex)
  
  # store the previous plotting settings:
  prevMfrow <- par('mfrow')
  
  # 
  if (!is.null(cols)){
    if (!(1 %in% cols)) cols <- c(1, cols)
    colPlot <- cols
    parNum <- length(cols) - 1
  }
  
  counter <- 1
  for(chain in chains){
    
    # set parNum, columns to plot and variable names:
    if(is.null(cols)){parNum <- dim(chain)[2]-1 ; colPlot <- 1:parNum}
    if (is.null(names)) names <- c("log Post", paste("var", 1:parNum))
    print(names)
    samps <-  dim(chain)[1]
    par(mfrow=c(parNum, 1))
    
    # plot trace:
    if (!is.null(filePath)) {
      pdf(paste(filePath, counter, ".PDF", sep=""), ...)
      par(mfrow=c(parNum, 1))
      counter <- counter + 1
    }
    
    try(apply(chain[,colPlot[-1]], 2, plotCol),silent = T)
    if (!is.null(filePath)) dev.off()
    
    # plot ACF:
    if (!is.null(filePath)){
      try(pdf(paste(filePath, counter, ".PDF", sep=""), ...), silent = T)
      par(mfrow=c(parNum, 1))
      counter <- counter + 1
    }
    
    try(apply(chain[,colPlot[-1]], 2, plotACF), silent = T)
    if (!is.null(filePath)) dev.off()
    
    # plot joint posteriors:
    class(chain) <- "matrix"
    df <- data.frame(chain[runif(1000,1,nrow(chain)), colPlot])
    names(df) <- names
    
    if (!is.null(filePath)) {
      try(pdf(paste(filePath, counter, ".PDF", sep=""), ...), silent = T)
      par(mfrow=c(parNum, 1))
      counter <- counter + 1
    }
    
    plot(df, pch=16,col=adjustcolor('black', alpha=0.2), cex.lab=cex)
    if (!is.null(filePath)) dev.off()
  }
  
  # reset the graphical parameters to what they were before the call:
  par(mfrow=prevMfrow)
}

thinMCMC <- function(pMCMCoutput, alpha=0.1, removeBI=FALSE){
  # purpose : takes the output of the pHM function, and thins the chain to the
  #           desired level
  # inputs  : pMCMCMoutput - the output list of chains from the pMH function
  #           alpha        - the percentage of points to keep
  #           removeBI     - a number between 0 and 1 which indicates how far 
  #                          through the chain the burn in ends
  
  nchains <- length(pMCMCoutput)
  
  for (i in 1:nchains){
    chain <- pMCMCoutput[[i]]
    if (!is.null(removeBI)){
      pMCMCoutput[[i]] <- chain[round(nrow(chain)*removeBI):nrow(chain),]
    }
    
    n <- nrow(pMCMCoutput[[i]])
    toKeep <- round(seq(1,n,length=round(alpha*n)))
    pMCMCoutput[[i]] <- pMCMCoutput[[i]][toKeep,]
  }
  
  return(pMCMCoutput)
}

getAcceptance <- function(pMCMCoutput, start = 1, end = NULL){
  # purpose : Calculates each chain's acceptance probability
  # inputs  : pMCMCoutput - the output list of chains from the pMH function
  #           start       - the sample at which we start considering the
  #                         acceptance probability
  # output  : A numeric vector with the acceptance proability of each chain
  
  n <- length(pMCMCoutput)
  output <- rep(NA, n)
  
  for (i in 1:n){
    chain <- pMCMCoutput[[i]]
    N <- nrow(chain)
    if (is.null(end)) end <- N
    output[i] <- chain[start:end,1] %>% diff %>% `!=`(0) %>% sum %>% `/`(N)
  }
  
  return(output)
}

# grDiagnostic <- function(chains,plotty=T){
#   # purpose : Uses the CODA library to conduct a Gelman-Rubin test for MCMC
#   #           convergence
#   # inputs  : A list conatining the reult of the MCMC chains, assuming that
#   #           the first column gives the value of the log-posterior
#   # output  : Plot of the G-R convergence diagnostic test statistic
#   require(coda)
#   thinnednl <- lapply(chains, function(x) x[,-1])
#   combined <- thinnednl %>% lapply(FUN=mcmc) %>% do.call(what=mcmc.list)
#   if(plotty) return(gelman.plot(combined))
#   return(combined)
# }

######################################################################################

Nested_MH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
                      clNeeds, RcppFile=NULL, prntPars=FALSE, packages = "Rcpp", cores=detectCores()){
  # purpose : wrapper function which runs MCMC chains in parallel
  # inputs  : nChains   - The number of chains which should be run
  #           clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  # source any required files:
  if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
  
  itermax<-ceiling(itermax/cores)*cores+1L
  xPrev<-x0
  n <- length(xPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  xNew <- matrix(NA, nrow=cores, ncol=n-1)
  lTargNew<-alpha<-c()
  
  # Add the initial point, the first column of the output matrix is an 
  # evaluation of the log Target at the sampled point:
  output[1, ] <- c(lTarg(xPrev, lTargPars), xPrev)
  print(output[1, ])
  #print("Initial Value in Nested_MH = ");print(output[1,])
  
  # Create the clusters and initiate paralellisation:
  cl<-makeCluster(cores,outfile="./Rcode/OUTPUT.txt")
  registerDoParallel(cl)
  clusterExport(cl, clNeeds)
  
  it <- 2
  while (it <= itermax){
    
    # if(any((it:(it+cores)*100)%%itermax==0)){
    print(paste0(round(it*100/itermax),"% done. LL = ",output[it-1,1]))
    #   saveRDS(output,file=paste0("./Rcode/saver_",round(it*100/itermax),"percent.Rdata"))
    # }
    
    # generate a proposed point:
    for (c in 1:cores){
      xNew[c,] <- proposal(xPrev, propPars) 
    }
    
    # calculate the acceptance probability:
    lTargNew <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
      
      to.lTargNew <- tryCatch({lTarg(xNew[c,], lTargPars)}, error=function(e) NA)
    }
    
    lTargOld <- output[it-1, 1]
    alpha <- exp(lTargNew - lTargOld)
    
    print("alpha")
    print(alpha)
    print("")
    
    # determine acceptance or rejection:
    u <- runif(cores)
    u2 <- runif(cores)
    lTtmp<-0
    
    for (c in 1:cores){
      # Check that the log-likelihood was actually calculated
      if(is.na(lTargNew[c])) {output[it+c-1,] <- output[it+c-2,] ; next}
      # MH acceptance-rejection:
      if (u[c]<=alpha[c] & exp(lTargNew[c]-lTtmp)>=u2[c]) {
        output[it+c-1,] <- c(lTargNew[c], xNew[c,])
        lTtmp<-lTargNew[c]
      }
      else output[it+c-1,] <- output[it+c-2,]
    }
    
    # update Xt-1
    xPrev <- output[it+cores-1,2:n]
    
    # update iterator:
    it <- it + cores
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  try(stopCluster(cl), silent = T)
  return(output)
  
}

Nested_pMH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
                       uFunc=NULL, prntPars=FALSE, clNeeds, RcppFile=NULL,
                       packages = "Rcpp", cores=detectCores()){
  
  if(itermax<200) uFunc<-NULL
  
  RNGkind("L'Ecuyer-CMRG")
  set.seed(201020)
  
  if (is.null(uFunc)){
    chain <- Nested_MH(proposal = proposal, propPars = propPars, lTarg = lTarg, 
                       lTargPars = lTargPars, x0 = x0, itermax = itermax,
                       clNeeds = clNeeds, RcppFile = RcppFile, packages = packages, cores = cores)
    return(chain)
  }
  
  # otherwise, we perform adaptive MCMC:
  else{
    output <- NULL
    
    # We perform the MCMC in three phases. An inital warmup phase (lasting 25%
    # of itermax), follOwed by an adaptation phase, where we use the warmup
    # samples to modify our proposal, sample 25% of itermax more samples and
    # then modify the proposal again. The final stage uses the newest proposal
    # to produce samples for the last 50% of itermax iterations
    
    # Split itermax into 2 groups of 25% and one group of 50%:
    div <- c(round((itermax-1)/4))
    indices <- c(div, div, itermax-2*div)
    covy<-1
    
    for (im in indices){
      # Optionally display the proposal parameters:
      # if (prntPars) print(propPars)
      
      # run the chain:
      chain <- Nested_MH(proposal = proposal, propPars = propPars, lTarg = lTarg, 
                         lTargPars = lTargPars, x0 = x0, itermax = im,
                         clNeeds = clNeeds, RcppFile = RcppFile, packages = packages, cores = cores)
      # add the samples to the output matrix
      output <- rbind(output, chain)
      
      # This line simply breaks out the loop if we don't have a following 
      # phase, to avoid the computational cost of updating our parameters:
      if(dim(output)[1]>=itermax) break
      
      # update the proposal parameters for the next phase:
      chain%<>%na.omit
      # To automate 
      # ichain <- sample(x = 1:nrow(chain), size = round(0.5*nrow(chain)), replace = F, prob = exp(chain[,1]-max(chain[,1])))
      # propPars <- uFunc(chain[ichain,])
      propPars <- uFunc(chain)
      
      print(paste0("COV Matrix - Max diagonal element :",round(max(diag(propPars)),digits = 3)))
      print(paste0("COV Matrix - Max off-diagonal element :",round(max(propPars - diag(diag(propPars))),digits = 3)))
      
      saveRDS(propPars,file=paste0("./saver_COV_",im,"_",covy,".Rdata"))
      covy<-covy+1
      
      # update xPrev for the next phase, using a random sample of one of the values used to calculate the covariance matrix 
      # xPrev <- chain[sample(ichain,1),-1]
      xPrev <- chain[im,-1]
      # Choose the next initial values to have the largest log-likelihood.
      # xPrev <- output[which.max(output[,1]),-1]
    }
    return(output)
  }
  
}

pM_GaA <- function(propCOV, lTarg, lTargPars, x0, itermax=1000,
                   prntPars=FALSE, clNeeds, RcppFile=NULL,
                   packages = "Rcpp", cores=detectCores(), 
                   Params=list(GreedyStart=500,Pstar=0.234, gamzy0=NA, epsilon=1, minVar=1e-9)){
  # purpose : Generalised Adaptative Metropolis-Hastings Algorithm with Global Adaptative Scaling
  # Details : Uses the multivariate Gaussian distribution as proposal, 
  #           whereby the proposal covariance is updated at every iteration.
  # inputs  : clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  # Check: 
      # Get rid of link functions and replace with rejection routine
  
  set.seed(round(runif(1,0,10000)))
  # Setup the MCMC algorithm parameters
  xPrev<-propMu<-x0
  if(is.na(Params$gamzy0)) Params$gamzy0<-2.38^2/length(x0)
  gamzy <- Params$gamzy0/(1:(itermax+cores))^(seq(from=1/(1+Params$epsilon),to=1,length.out=(itermax+cores)))
  eps<-Params$minVar/(1:(itermax+cores))
  Lgsf<-0
  C_0<-propCOV
  
  # source any required files:
  if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
  
  # Initialisations of arrays
  itermax<-ceiling(itermax/cores)*cores+1L
  n <- length(xPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  xNew <- matrix(NA, nrow=cores, ncol=n-1)
  lTargNew<-alpha<-c()
  
  # Add the initial point, the first column of the output matrix is an 
  # evaluation of the log Target at the sampled point:
  lTargOld <- lTarg(xPrev, lTargPars)
  output[1, ] <- c(lTargOld, xPrev)
  print(output[1,1])
  it <- 2
  while (it <= itermax){
    # Generate a proposed point using the MVN proposal and global scaling factor
    # Note that GreedyStart corresponds to a form of annealing
    if(it>Params$GreedyStart) {xNew <- multvarNormProp(xt=xPrev, propPars=exp(2*Lgsf)*propCOV, n=cores) 
    } else xNew <- multvarNormProp(xt=xPrev, propPars=C_0, n=cores) 
    # Sample from the target distribution
    lTargNew <- unlist(mclapply(X = 1:cores,
                                FUN = function(c) lTarg(xNew[c,], lTargPars),
                                mc.cores = cores))
    # Acceptance ratios
    alpha <- apply(cbind(exp(lTargNew - lTargOld),rep(1,cores)),1,min)*
             vapply(1:cores,function(c) modifyAcc(xPrev,xNew[c,]),FUN.VALUE = numeric(1))
    # print(paste0("<alpha> = ",mean(alpha)))
    # determine acceptance or rejection:
    u <- runif(cores)
    u2 <- runif(cores)
    lTtmp<-lTargOld
    # Acceptance/rejection routine
    for (c in 1:cores){
      # Check that the log-likelihood was actually calculated
      if(is.na(lTargNew[c])) {output[it+c-1,] <- output[it+c-2,]; next}
      # Store these values
      output[it+c-1,] <- c(lTargNew[c], xNew[c,])
      # Acceptance/rejection
      if (exp(lTargNew[c]-lTtmp)>=u[c]) { # Accepted!
        lTtmp<-lTargNew[c]
        xPrev<-xNew[c,]
      }
      # Adaptive component of MCMC algorithm
      if(it>Params$GreedyStart) {
        # For stochastic, likelihood free models, avoid falling into ficticious local minima
        sigLL<-1.-2*pnorm(lTargNew[c]-lTtmp,mean = 0,sd = 1.2)
        # Update the Global Scaling Factor (GSF), mean & covariance
        Lgsf <- Lgsf + sigLL*gamzy[it+c-1]*(alpha[c]-Params$Pstar)
        propMu <- propMu + sigLL*gamzy[it+c-1]*(xNew[c,] - propMu)
        propCOV <- propCOV + sigLL*gamzy[it+c-1] * ((xNew[c,] - propMu) %*% t(xNew[c,] - propMu) - propCOV) + eps[it+c-1]
      }
    }
    # Update target value for acceptance ratio at next step
    lTargOld<-lTtmp
    # Print checks
    # print(paste0("Total covariance = ",sum(exp(2*Lgsf)*propCOV)))
    # print(paste0("Global Scaling Factor, r = ",exp(2*Lgsf)))
    print(paste0(round(it*100/itermax),"% done. LL = ",output[it+c-1,1]))
    # update iterator:
    it <- it + cores
  }
  
  return(output)
  
}

p_HMC <- function(propCOV, lTarg, lTargPars, x0, itermax=1000,
                  prntPars=FALSE, clNeeds, RcppFile=NULL,
                  packages = "Rcpp", cores=detectCores(), 
                  Params=list(Pstar=0.65, Leaps=20, stepsize=0.0005, gamzy0=1, egam=2, minVar=1e-3)){
  # purpose : Generalised Adaptative Metropolis-Hastings Algorithm with Global Adaptative Scaling
  # Details : Christian L. Muller, (ETH) 'Exploring the common concepts of a-MCMC and cov matrix
  #           adaptation schemes', Dagstuhl Seminar Proceedings, 2010.
  #           Uses the multivariate Gaussian distribution as proposal, 
  #           whereby the proposal covariance is updated at every iteration.
  # inputs  : clNeeds   - The character names of functions which need to be 
  #                       exported to the cluster.
  #           RcppFile  - The Rcpp file which must be fourced for required
  #                       Rcpp code to work
  #           other     - for all other inputs, refer to the description given
  #                       in the MH function.
  # output  : A list containing the output matrices of all chains
  # note : loads Rcpp onto the cluster by default.
  
  ############## HAMILTONIAN MONTE-CARLO ALGORITHM #############
  # For more information, see either: 
  #           https://arxiv.org/pdf/1701.02434.pdf
  #           https://www.mcmchandbook.net/HandbookChapter5.pdf
  ### FOR EACH ITERATION 
  # Generate new p values
  # Sample new q values
  ### HMC-loop (leapfrog steps)
  # Propagate q & p (calculating dV/dq each time)
  ###
  # Reverse p
  # Calculate new Hamiltonian
  # Calculate old Hamiltonian
  # MH accept-reject
  # Adjust propCOV to ensure Pstar = 0.65
  ###
  ##############################################################
  # Define kinetic energy function - Kin - we use quadratic with parameter scaling metric:
  KinF  <- function(p, propCOV){
    # NOTE: negative log term corresponds to taking 1/det(propCOV)
    # propCOV = M^(-1) for M, the Euclidean metric (also called mass matrix)
    return(-0.5*sum(p%*%propCOV*p) - log(det(propCOV)))
    # return(-0.5*sum(p*propCOV*propCOV*p) - log(prod(propCOV*propCOV)))
  }
  # Define potential energy function - Vpot - we use logTargetIPM:
  VpotF <- function(q){
    return(-lTarg(q,lTargPars))
  }
  # Define the Hamiltonian - Ham
  HamF <- function(q,p,q_old=NULL,p_old=NULL,propCOV){
    # LL is actually -Vpotential, see output
    if(is.null(q_old)|is.null(p_old)) return(list(new=(VpotF(q)+KinF(p,propCOV))))
    return(list(new=(VpotF(q)+KinF(p,propCOV)),old=(VpotF(q_old)+KinF(p_old,propCOV))))
  }
  # Define derivative of potential dV/dq:
  dVpotF <- function(q,delta=0.0001){
    n<-length(q)
    dVpot<-rep(0,n)
    
    for (i in 1:n){
      delvec<-rep(0,n)
      delvec[i]=delta
      
      lower<-VpotF(q-delvec)
      upper<-VpotF(q+delvec)
      # print(mean(c(lower,upper)))
      
      dVpot[i]<-upper-lower
    }
    
    return(dVpot=dVpot/(2*delta))
  }
  # Define Leapfrog algorithm
  LeapfrogMe <- function(q,p,Leaps=20,eps=0.0005){
    p%<>%as.numeric()
    q%<>%as.numeric()
    for (L in 1:Leaps){
      # Half step in momentum
      dV1<-dVpotF(q)
      print("dVpot 1:")
      print(dV1)
      p<-p-0.5*eps*dV1
      print("0.5*eps*dV1:")
      print(0.5*eps*dV1)
      print("eps*p")
      print(eps*p)
      # Update position
      q<-q+eps*p
      print("q*")
      print(q)
      # Second half-step in momentum
      dV2<-dVpotF(q)
      print("dVpot 2:")
      print(dV2)
      p<-p-0.5*eps*dV2
    }
    return(list(q=q,p=-p))
  }
  
  # Add these functions to the required parallel functions (see ClusterExport)
  clNeeds%<>%c("lTarg","dVpotF","HamF","VpotF","KinF","LeapfrogMe")
  
  #%%%%%%%%%%%%%%%% WHEN TO PARALLELISE????? %%%%%%%%%%%%%%%%%#
  # Do they share propCOV? Do we make propCOV an average/median of the others?
  # Deal with step size and number of steps
  
  RNGkind("L'Ecuyer-CMRG")
  set.seed(201020)
  # source any required files:
  if (!is.null(RcppFile)) Rcpp::sourceCpp(RcppFile)
  itermax<-ceiling(itermax/cores)*cores+1L
  qPrev<-x0
  n <- length(qPrev) + 1
  output <- matrix(NA, nrow=itermax, ncol=n)
  qNew <- matrix(NA, nrow=cores, ncol=n-1)
  # Create the clusters and initiate paralellisation:
  # cl<-makeCluster(cores,outfile="./Rcode/OUTPUT.txt")
  # registerDoParallel(cl)
  # clusterExport(cl, clNeeds)
  
  ### Adaptative accept-reject algorithm parameters, evolving covariance matrix ###
  propMu<- x0
  gamzy <- Params$gamzy0/(1:(itermax+cores))^(seq(from=1/(1+Params$egam),to=1,length.out=(itermax+cores)))
  Lgsf<-0
  Pstar<-Params$Pstar
  
  pInit<-multvarNormProp(rep(0,(n-1)), exp(2*Lgsf)*propCOV, n=cores)
  # Print out LL & summary statistics of the initial parameters
  print(-VpotF(qPrev))
  print(HamF(q = qPrev,p = pInit,propCOV = propCOV))
  Leapy<-LeapfrogMe(q = qPrev,p = pInit,Leaps = 3,eps = sqrt(diag(propCOV)))
  
  print("sum(<COV,COV>):")  
  print(sum(propCOV*t(propCOV)))
  print("")
  
  it <- 1
  while (it <= itermax){
    
    # Generate momentum variables:
    pCur <- multvarNormProp(rep(0,(n-1)), exp(2*Lgsf)*propCOV, n=cores)
    # pNew <- array(rnorm((n-1)*cores,mean=0,sd=1),dim=c((n-1),cores))*s
    
    # Generate a proposed point using the MVN proposal and global scaling factor
    qCur <- qPrev
    # qCur <- multvarNormProp(qPrev, exp(2*Lgsf)*propCOV, n=cores) 
    
    # calculate the HMC new position-momentum values:
    # listQPspace <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
    #   to.listQPspace <- tryCatch({LeapfrogMe(qCur[c,],pCur[c,],Leaps = Params$Leaps,
    #                                          eps = Params$stepsize)}, error=function(e) NA)
    # }
    listQPspace<-tryCatch({LeapfrogMe(qCur,pCur,Leaps = Params$Leaps,
                                      eps = sqrt(diag(propCOV)))}, error=function(e) NA)
    print("3")
    print(listQPspace)
    qNew<-listQPspace$q
    # NOTE: momentum is reversed for volume preservation!
    pNew<-listQPspace$p
    print("4")
    # Calculate the proposed and current Hamiltonian values
    # list_Ham <- foreach(c=1:cores, .packages = packages, .combine = c) %dopar%{
    #   to.list_Ham <- tryCatch({HamF(qNew[c,],pNew[c,],qCur[c,],pCur[c,],propCOV)}, error=function(e) NA)
    # }
    list_Ham<-tryCatch({HamF(qNew,pNew,qCur,pCur,propCOV)}, error=function(e) NA)
    Hamiltonian <- list_Ham$new
    o_Hamiltonian <- list_Ham$old
    print("5")
    # MH accept-reject parameter
    alpha <- apply(cbind(exp(-Hamiltonian + o_Hamiltonian),rep(1,cores)),1,min)
    print("<alpha> = ")
    print(alpha)
    print("")
    
    # determine acceptance or rejection:
    u <- runif(cores)
    
    tLgsf<-tpropMu<-tpropCOV<-nnz<-0
    HamTmp<-min(o_Hamiltonian)
    
    for (c in 1:cores){
      # Check that the log-likelihood was actually calculated
      if(is.na(Hamiltonian[c])) {
        output[it+c-1,] <- output[it+c-2,]
        nnz<-nnz+1
        next
      }
      
      # Note: modifying global scale factor by alpha, not exp(-Ham[c]+HamTmp)
      tLgsf <- tLgsf + gamzy[it]*(alpha[c]-Pstar)
      tpropMu <- tpropMu + gamzy[it]*(qCur[c,] - propMu)
      tpropCOV <- tpropCOV + gamzy[it]*((qCur[c,] - propMu)%*%t(qCur[c,] - propMu) - propCOV)
      
      # MH acceptance-rejection:
      if (exp(-Hamiltonian[c]+HamTmp)>=u[c]) {
        output[it+c-1,] <- c(Hamiltonian[c], qNew[c,])
        HamTmp<-Hamiltonian[c]
      } else output[it+c-1,] <- output[it+c-2,]
      
    }
    
    # # if(nnz<cores & it>Params$burnin){
    # if(nnz<cores){
    #   # Ensure a minimum variance value, even despite potentially low acceptance rate.
    #   diag(tpropCOV)<-diag(tpropCOV)+Params$minVar
    #   
    #   Lgsf<-Lgsf+tLgsf/max(c(cores-nnz,1))
    #   propMu<-propMu+tpropMu/max(c(cores-nnz,1))
    #   propCOV<-propCOV+tpropCOV/max(c(cores-nnz,1))
    # }
    
    # print("sum(<COV,COV>):")  
    # print(sum(propCOV*t(propCOV)))
    # print("Global Scaling Factor, r:")
    # print(exp(Lgsf))
    # print("")
    # if(any((it:(it+cores)*100)%%itermax==0)){
    print(paste0(round(it*100/itermax),"% done. LL = ",output[it,1]))
    print(output[it,-1])
    print("")
    # saveRDS(output,file=paste0("./Rcode/saver_",round(it*100/itermax),"percent.Rdata"))
    # }
    
    # update Xt-1
    qPrev <- output[it+cores-1,-1]
    
    # update iterator:
    it <- it + cores
  }
  
  # Do the tear down for the parallelisation, we exception handle it simply
  # to silence a bunch of warnings that indicate which connections to unused
  # ports are closed by stopping the cluster
  # try(stopCluster(cl), silent = T)
  return(output)
  
}

# Find some quantiles of the accepted parameter space values
multiQuants<-function(x,lenny=300,qmin=0.15,qmax=0.85){
  # Find the min and max values, per parameter, to create a grid from
  range<-vapply(1:ncol(x),function(i) quantile(x,c(qmin,qmax),T,F), numeric(2))
  # Output the grid of values!
  vapply(1:ncol(x),function(i) seq(range[1,i],range[2,i],length.out=lenny),numeric(lenny))
}

# To adjust the ABC-threshold, we calculate the supremum of the ratio of two posteriors (at time t and t-1)
# U. Simola, et al, 'Adaptive Approximate Bayesian Computation Tolerance Selection', Bayesian Anal. 16(2): 397-423 (June 2021). DOI: 10.1214/20-BA1211 
Supremum<-function(q_old,xNew,xPrev,xN=T,meth="KLIEP",warny=T){
  # Calculate the density ratio using Kullback-Leibler Importance Estimation Procedure (KLIEP) - https://github.com/hoxo-m/densratio
  densratio_obj <- densratio(xNew,xPrev,method = meth,verbose = warny)
  # Which parameter space values should have quantiles?
  if(xN) xxx<-xNew else xxx<-rbind(xNew,xPrev)
  # Values to predict on, based on quartiles of only the new parameters
  x_quants<-multiQuants(xxx)
  # Compute the ratio for the range of prediction values
  w_hat <- densratio_obj$compute_density_ratio(x_quants)
  # Return the updated supremum*, as a vector that includes the older values
  return(c(1/max(w_hat),q_old))
}

# Generate Np accepted particles (sets of model-parameters)
# given the ABC-threshold (delta), target & resample functions and initial values
GenAccSamples<-function(delta, initSIR, lTarg, lTargPars, ResampleSIR){
  # How many particles do we want?
  particles<-initSIR$Np; Complete<-0
  # Output skeleton
  output<-list(delta=delta,distance=c(),theta=array(dim = c(0,length(initSIR$x0))),
               weightings=c(),shat=array(dim = c(0,length(lTargPars$SumStats))))
  # Sample from parameter space until we have Np accepted particles
  while (particles>0){
    # Sample the particles (note that the weightings are dynamically defined along with the function)
    SIR<-ResampleSIR(max(particles,lTargPars$cores))
    # Sample from the target distribution
    lTargNew <- mclapply(X = 1:max(particles,lTargPars$cores),
                         FUN = function(c) lTarg(SIR$theta[c,], lTargPars),
                         mc.cores = lTargPars$cores) %>% CombLogTargs()
    # Modify which particles are sampled from at next iteration
    Complete<-sum(lTargNew$d>delta)+Complete
    # Save both accepted & rejected values
    output$distance%<>%c(lTargNew$d)
    output$theta%<>%rbind(SIR$theta)
    output$weightings%<>%c(SIR$weightings)
    output$shat%<>%rbind(lTargNew$shat)
    # Adjust the number of particles required for the next iteration
    particles<-initSIR$Np-Complete
    # print out
    print(paste0(particles," out of ",initSIR$Np," left to simulate"))
  }
  # Set the weights of rejected particles to zero
  output$weightings[output$distance<output$delta]<-0
  # Normalise the weights
  output$weightings<-output$weightings/sum(output$weightings)
  
  return(output)
}

# Initialise the first round of the ABCSMC algorithm
InitABCSIR<-function(lTarg, lTargPars, initSIR){
  # Total number of parameter-space particles to run through particle filter
  Ninit<-initSIR$Np*initSIR$k
  # Generate the particles
  xNew<-multvarNormProp(xt=initSIR$x0, propPars=initSIR$propCOV, n=Ninit)
  # Calculate the priors for the weights, and normalise them
  wtwt<-exp(apply(xNew,1,function(tt) lTargPars$priorF(tt))); wtwt<-wtwt/sum(wtwt)
  # Sample from the target distribution
  lTargNew <- mclapply(X = 1:Ninit,
                       FUN = function(c) lTarg(xNew[c,], lTargPars),
                       mc.cores = lTargPars$cores) %>% CombLogTargs()
  # Calculate the initial ABC-threshold required for the ABCSIR algorithm
  delta0<-lTargNew$d[order(lTargNew$d,decreasing = T)[initSIR$Np]]
  # Output both accepted & rejected values
  outy<-list(distance=lTargNew$d,
             theta=xNew,
             weightings=wtwt,
             shat=lTargNew$shat,
             delta=delta0)
  
  return(outy)
}

# Define the function that will be used to resample the SIR-particles
defFsamp<-match.fun(perturber)

CalcQuantile<-function(output,xPrev){
  # BLOODY DENSITY RATIO ALGORITHM IS A PAIN IN MY ARSE!
  q_thresh<-output$q_thresh[output$iteration]
  q_update<-tryCatch(Supremum(q_thresh,output$theta[output$distance>output$delta,],xPrev),error=function(e) NA)
  if(is.na(q_update)) {q_update<-tryCatch(Supremum(q_thresh,xPrev,output$theta[output$distance>output$delta,],F),error=function(e) NA); print("Trying xNew & xPrev quantiles only")}
  if(is.na(q_update)) {q_update<-tryCatch(Supremum(q_thresh,xPrev,output$theta[output$distance>output$delta,],T,meth = "RuLSIF"),error=function(e) NA); print("Trying RuLSIF")}
  if(is.na(q_update)) {q_update<-tryCatch(Supremum(q_thresh,xPrev,output$theta[output$distance>output$delta,],T,meth = "RuLSIF"),error=function(e) NA); print("Trying RuLSIF and xNew & xPrev quantiles only")}
  if(is.na(q_update)) {q_update<-tryCatch(Supremum(q_thresh,xPrev,output$theta[output$distance>output$delta,]),error=function(e) NA); print("Reversing KLIEP num/denom")}
  if(is.na(q_update)) {
    q_update<-c(q_thresh[length(q_thresh)],q_thresh)
    print("warning: issues with the quantile threshold calculation")
  } 
  return(q_update)
}

ModThresh<-function(output,xPrev){
  # Calculate the quantile function
  output$q_thresh<-CalcQuantile(output,xPrev)
  # Decrease the current ABC-threshold
  output$delta<-output$delta/output$q_thresh[output$iteration+1]
  
  return(output)
}

CalcESS<-function(output){
  # Note that in GenAccSamples we set all rejected weights to zero and normalise the rest
  1/sum(output$weightings^2)
}

# The ABCSIR (also referred to as ABCSMC) algorithm 
ABCSIR<-function(initSIR, lTarg, lTargPars){
  # Initialisations of storage variables and algorithm parameters
  xPrev<-initSIR$x0; indy<-1:length(xPrev); cycles<-initSIR$Np*initSIR$k
  # Find theta*(t=1) delta(t=1) -> the ABC-rejection value
  output<-InitABCSIR(lTarg, lTargPars, initSIR); output$iteration<-it<-1; output$q_thresh<-q_thresh<-c(0.95);
  # Setup shop for the full algorithm
  saveRDS(list(output),paste0("output_",namer))
  # Run the algorithm!
  while(!(output$q_thresh[it] > 0.99 & it>3) | cycles > initSIR$itermax){
    # Set the ABC-step number
    it<-it+1
    # Dynamically define the resample & perturb function of new parameter sets
    ResampleSIR<-defFsamp(outin = output,SumStats = c(lTargPars$SumStats),priorF = lTargPars$priorF)
    # SIR routine
    output<-GenAccSamples(output$delta, initSIR, lTarg, lTargPars, ResampleSIR)
    # Modify the ABC-threshold adaptively
    output<-ModThresh(output,xPrev)
    # Calculate the Effective Sample Size (ESS), noting that it is already normalised
    output$ESS<-CalcESS(output)
    # How many samples have we taken sum_t(D_t)?
    cycles<-cycles+nrow(output$theta); output$iteration<-it
    # Save previous parameter space samples
    xPrev<-output$theta[output$distance>=output$delta,]
    # Save the output!
    prev<-readRDS(paste0("output_",namer)); saveRDS(c(prev,list(output)),paste0("output_",namer)); rm(prev)
    # Tell me what's good... please, please, please
    print(paste0("Step = ",it,", No. samples = ",cycles,", eps = ",signif(output$delta,3),
                 " with 1/c = ",output$q_thresh[it]," and ESS = ",output$ESS))
    print("")
  }
  return(output)
}

# Maybe try out this algorithm as well?
# https://link.springer.com/article/10.1007/s00180-021-01093-4




